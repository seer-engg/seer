# ğŸ”® Seer - A2A Multi-Agent Evaluation Platform

**Seer** is a Multi-Agent System (MAS) for evaluating AI agents through blackbox testing.

Agents communicate via LangGraph's Agent-to-Agent (A2A) protocol with an orchestrator acting as a central hub, enabling modular, scalable, and traceable interactions.

---

## ğŸš€ Quick Start

```bash
# 1. Setup
./setup.sh

# 2. Start your agent (separate terminal)
cd /path/to/your/agent && langgraph dev --port 2024

# 3. Launch Seer 
python run.py

# 4. Open UI
# UI:                 http://localhost:8501
# Chat directly with the orchestrator, which delegates to eval/coding agents
# LangGraph Studio:   https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:8000
```

Real LangGraph agents with proper structure, testable in isolation with `langgraph dev`.

**See [ARCHITECTURE_CHOICE.md](ARCHITECTURE_CHOICE.md) for detailed comparison.**

---

## ğŸ“š Documentation

- **[ARCHITECTURE.md](ARCHITECTURE.md)** - System architecture and design
- **[DATABASE.md](DATABASE.md)** - Database schema and persistence layer
- **[Simulation Flow](Simulation%20Evaluating%20Vertex%20wo%20debugging.txt)** - Example evaluation flow

---

## ğŸ—ï¸ Architecture

```
User â†â†’ Streamlit UI â†â†’ Orchestrator Agent (Conversational)
                              (port 8000)
                                   â†“
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â†“                   â†“
                  Eval Agent          Coding Agent
                  (port 8002)         (port 8003)
                         
              A2A Point-to-Point Communication
```

**Why This Architecture:**
- âœ… **Conversational orchestrator** - Users interact directly with orchestrator
- âœ… **Point-to-point A2A** - No broadcast, only targeted agent delegation
- âœ… **Real agents** - Proper LangGraph structure with state, tools, workflows
- âœ… **Quick acknowledgment** - Orchestrator acknowledges and relays agent responses
- âœ… **Testable in isolation** - Use `langgraph dev` to test agents individually
- âœ… **Blackbox A2A testing** - Test your agent without accessing code
- âœ… **Persistent storage** - SQLite database stores all chat threads and eval data
- âœ… **Simplified deployment** - No bridge or customer success agent needed

**Agent Files:**
- Orchestrator: `agents/orchestrator/simplified_graph.py` (Conversational hub with A2A routing)
- Eval Agent: `agents/eval_agent/simplified_graph.py` (Test generation and execution)
- Coding Agent: `agents/coding_agent/graph.py` (Code analysis and review)

**Configuration:**
- `deployment-config.json` - Centralized agent UUIDs and ports
- `shared/config.py` - Configuration management utilities

---

## ğŸ“ Project Structure

```
seer/
â”œâ”€â”€ agents/             # LangGraph agents with A2A communication
â”‚   â”œâ”€â”€ orchestrator/           # Conversational orchestrator with A2A routing
â”‚   â”‚   â”œâ”€â”€ simplified_graph.py  # Main orchestrator logic
â”‚   â”‚   â””â”€â”€ langgraph.json
â”‚   â”œâ”€â”€ eval_agent/
â”‚   â”‚   â”œâ”€â”€ simplified_graph.py  # LangGraph agent graph
â”‚   â”‚   â””â”€â”€ langgraph.json
â”‚   â””â”€â”€ coding_agent/
â”‚       â”œâ”€â”€ graph.py             # LangGraph agent graph
â”‚       â””â”€â”€ langgraph.json
â”œâ”€â”€ shared/             # Shared utilities (schemas, prompts, database, config)
â”œâ”€â”€ data/               # SQLite database storage
â”œâ”€â”€ ui/                 # Streamlit UI
â”‚   â””â”€â”€ streamlit_app.py
â”œâ”€â”€ deployment-config.json  # Agent UUIDs and configuration
â”œâ”€â”€ run.py              # Launcher (starts everything)
```

---

## ğŸ’¬ Example Usage

```
You: "Evaluate my agent at http://localhost:2024 (ID: abc123).
     It should recall memories and respond politely."

Seer: âœ… Generated 6 tests. Ready to run?

You: "Yes"

Seer: ğŸ“Š Results: 5/6 passed (83%)
```

---

## ğŸ’¾ Database & Persistence

Seer uses SQLite to persist all data:
- **Chat threads** - All conversations with complete history
- **Messages** - Every message from users and agents
- **Agent activities** - What each agent did in each thread (for debugging/tracing)
- **Eval suites** - Generated test cases
- **Test results** - Detailed test execution results

---

## ğŸ“Š Monitoring & Debugging

### In Streamlit UI (Recommended)

Open http://localhost:8501 and use the tabs:

1. **ğŸ’¬ Chat** - Interact with the conversational orchestrator
   - Orchestrator handles user conversations directly
   - Automatically delegates to eval or coding agents as needed
2. **ğŸ“Š Results** - View evaluation results and test suites
   - See test case results
   - Track evaluation progress
   - View historical test runs

### LangGraph Studio (Browser-based)

Access LangGraph Studio for advanced debugging:
- **Orchestrator Agent**: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:8000
- **Eval Agent**: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:8002
- **Coding Agent**: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:8003

Features:
- Visual graph execution
- Real-time conversation monitoring
- Tool call inspection
- State management debugging
- A2A communication traces

### Via Log Files

```bash
# Orchestrator agent (LangGraph)
tail -f logs/orchestrator_langgraph.log

# Eval agent (LangGraph)
tail -f logs/eval_agent_langgraph.log

# Coding agent (LangGraph)
tail -f logs/coding_agent_langgraph.log
```

**What you'll see in logs:**
- ğŸ›ï¸ Orchestrator conversations and A2A delegation
- ğŸ¤– Point-to-point A2A communication traces
- ğŸ“¨ Agent-specific activity with tool calls and responses
- ğŸ”„ Agent registration and status updates

---

## ğŸ› ï¸ Development

**Add a new agent:**
1. Copy `agents/eval_agent/` as template
2. Define state, tools, and workflow
3. Add agent to `deployment-config.json` with UUID and port
4. Add delegation tool in orchestrator for the new agent
5. Update `run.py` to launch it

**Add new data types:**
1. Add schemas in `shared/schemas.py`
2. Update Orchestrator's data_manager to handle the new data types
3. Add tools in orchestrator for storing/retrieving the new data

---

## ğŸ™ Built With

- **LangGraph** - All agents (Orchestrator, Customer Success, Eval)
- **LangChain** - LLM orchestration
- **Streamlit** - UI
- **OpenAI** - LLM models
- **SQLite** - Data persistence

---

**Questions? Check [README_FULL.md](README_FULL.md)** ğŸ”®

