# ğŸ”® Seer - Multi-Agent System for Evaluating AI Agents

**Seer** is a Multi-Agent System (MAS) for evaluating AI agents through blackbox testing.

---

## ğŸš€ Quick Start

```bash
# 1. Setup PostgreSQL
# Quick: docker run --name seer-postgres -e POSTGRES_DB=seer_db -e POSTGRES_USER=seer_user -e POSTGRES_PASSWORD=seer_password -p 5432:5432 -d postgres:15

# 2. Configure .env
# Add: OPENAI_API_KEY=your_key
#      POSTGRESQL_CONNECTION_STRING=postgresql://seer_user:seer_password@localhost:5432/seer_db

# 3. Install dependencies
./setup.sh

# 4. Start your agent (separate terminal)
cd /path/to/your/agent && langgraph dev --port 2024

# 5. Launch Seer 
python run.py

# 6. Open UI
# UI:                 http://localhost:8501
# Chat directly with the orchestrator, which delegates to eval/coding agents
# LangGraph Studio:   https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:8000
```

**Agent Files:**
- Orchestrator: `agents/orchestrator/graph.py` (Conceirge)
- Eval Agent: `agents/eval_agent/graph.py` (Test generation and execution)
- Coding Agent: `agents/coding_agent/graph.py` (Code analysis and review)

**Configuration:**
- `deployment-config.json` - Centralized agent UUIDs and ports
- `shared/config.py` - Configuration management utilities

---

## ğŸ“ Project Structure

```
seer/
â”œâ”€â”€ agents/             # LangGraph agents with inter agent communication
â”‚   â”œâ”€â”€ orchestrator/           # Conversational orchestrator with inter agent communication
â”‚   â”‚   â”œâ”€â”€ graph.py  # Main orchestrator logic
â”‚   â”‚   â””â”€â”€ langgraph.json
â”‚   â”œâ”€â”€ eval_agent/
â”‚   â”‚   â”œâ”€â”€ graph.py  # LangGraph agent graph
â”‚   â”‚   â””â”€â”€ langgraph.json
â”‚   â””â”€â”€ coding_agent/
â”‚       â”œâ”€â”€ graph.py             # LangGraph agent graph
â”‚       â””â”€â”€ langgraph.json
â”œâ”€â”€ shared/             # Shared utilities (schemas, prompts, database, config)
â”œâ”€â”€ data/               # SQLite database storage
â”œâ”€â”€ ui/                 # Streamlit UI
â”‚   â””â”€â”€ streamlit_app.py
â”œâ”€â”€ deployment-config.json  # Agent UUIDs and configuration
â”œâ”€â”€ run.py              # Launcher (starts everything)
```

---

## ğŸ’¬ Example Usage

```
You: "Evaluate my agent at http://localhost:2024 (ID: abc123).
     It should recall memories and respond politely."

Seer: âœ… Generated 6 tests. Ready to run?

You: "Yes"

Seer: ğŸ“Š Results: 5/6 passed (83%)
```

---

## ğŸ’¾ Database & Persistence

Seer uses **PostgreSQL with Peewee ORM** for scalable, production-ready data persistence:
- **Chat threads** - All conversations with complete history
- **Messages** - Every message from users and agents
- **Agent activities** - What each agent did in each thread (for debugging/tracing)
- **Eval suites** - Generated test cases
- **Test results** - Detailed test execution results

---

## ğŸ“Š Monitoring & Debugging

### In Streamlit UI (Recommended)

Open http://localhost:8501 and use the tabs:

1. **ğŸ’¬ Chat** - Interact with the conversational orchestrator
   - Orchestrator handles user conversations directly
   - Automatically delegates to eval or coding agents as needed
2. **ğŸ“Š Results** - View evaluation results and test suites
   - See test case results
   - Track evaluation progress
   - View historical test runs

### LangGraph Studio (Browser-based)

Access LangGraph Studio for advanced debugging:
- **Orchestrator Agent**: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:8000
- **Eval Agent**: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:8002
- **Coding Agent**: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:8003

Features:
- Visual graph execution
- Real-time conversation monitoring
- Tool call inspection
- State management debugging
- Inter agent communication traces

### Via Log Files

```bash
# Orchestrator agent (LangGraph)
tail -f logs/orchestrator_langgraph.log

# Eval agent (LangGraph)
tail -f logs/eval_agent_langgraph.log

# Coding agent (LangGraph)
tail -f logs/coding_agent_langgraph.log
```