# ğŸ”® Seer - A2A Multi-Agent Evaluation Platform

**Seer** is a Multi-Agent System (MAS) for evaluating AI agents through blackbox testing.

Agents communicate via LangGraph's Agent-to-Agent (A2A) protocol with an orchestrator acting as a central hub, enabling modular, scalable, and traceable interactions.

---

## ğŸš€ Quick Start

```bash
# 1. Setup
./setup.sh

# 2. Start your agent (separate terminal)
cd /path/to/your/agent && langgraph dev --port 2024

# 3. Launch Seer 
python run.py

# 4. Open UI
# UI:                 http://localhost:8501
# Use the "Orchestrator Monitor" tab to see message flow between all agents
# LangGraph Studio:   https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:8001
```

Real LangGraph agents with proper structure, testable in isolation with `langgraph dev`.

**See [ARCHITECTURE_CHOICE.md](ARCHITECTURE_CHOICE.md) for detailed comparison.**

---

## ğŸ“š Documentation

- **[ARCHITECTURE.md](ARCHITECTURE.md)** - System architecture and design
- **[DATABASE.md](DATABASE.md)** - Database schema and persistence layer
- **[Simulation Flow](Simulation%20Evaluating%20Vertex%20wo%20debugging.txt)** - Example evaluation flow

---

## ğŸ—ï¸ Architecture

```
Your Agent (Blackbox A2A)
        â†“
Customer Success Agent â†â†’ Orchestrator Agent â†â†’ Eval Agent
    (port 8001)         (port 8000)         (port 8002)
        â†‘                      â†‘                      â†‘
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     A2A Protocol (Hub & Spoke)
```

**Why This Architecture:**
- âœ… **Real agents** - Proper LangGraph structure with state, tools, workflows
- âœ… **Central coordination** - Orchestrator agent acts as message router and data store
- âœ… **Testable in isolation** - Use `langgraph dev` to test agents individually
- âœ… **Blackbox A2A testing** - Test your agent without accessing code
- âœ… **Full tracing** - See all message flow through the Orchestrator Monitor
- âœ… **Persistent storage** - SQLite database stores all chat threads and eval data
- âœ… **Simplified deployment** - No bridge processes needed

**Agent Files:**
- Orchestrator: `agents/orchestrator/graph.py` (Central hub)
- Customer Success: `agents/customer_success/graph.py`
- Eval Agent: `agents/eval_agent/graph.py`

**Configuration:**
- `deployment-config.json` - Centralized agent UUIDs and ports
- `shared/config.py` - Configuration management utilities

---

## ğŸ“ Project Structure

```
seer/
â”œâ”€â”€ agents/             # LangGraph agents with A2A communication
â”‚   â”œâ”€â”€ orchestrator/           # Central coordinating agent (group chat hub)
â”‚   â”‚   â”œâ”€â”€ graph.py             # Main orchestrator logic
â”‚   â”‚   â””â”€â”€ langgraph.json
â”‚   â”œâ”€â”€ customer_success/
â”‚   â”‚   â”œâ”€â”€ graph.py             # LangGraph agent graph
â”‚   â”‚   â””â”€â”€ langgraph.json
â”‚   â””â”€â”€ eval_agent/
â”‚       â”œâ”€â”€ graph.py             # LangGraph agent graph
â”‚       â””â”€â”€ langgraph.json
â”œâ”€â”€ shared/             # Shared utilities (schemas, prompts, database, config)
â”œâ”€â”€ data/               # SQLite database storage
â”œâ”€â”€ ui/                 # Streamlit UI
â”‚   â””â”€â”€ streamlit_app.py
â”œâ”€â”€ deployment-config.json  # Agent UUIDs and configuration
â”œâ”€â”€ run.py              # Launcher (starts everything)
â””â”€â”€ requirements.txt    # Dependencies
```

---

## ğŸ’¬ Example Usage

```
You: "Evaluate my agent at http://localhost:2024 (ID: abc123).
     It should recall memories and respond politely."

Seer: âœ… Generated 6 tests. Ready to run?

You: "Yes"

Seer: ğŸ“Š Results: 5/6 passed (83%)
```

---

## ğŸ’¾ Database & Persistence

Seer uses SQLite to persist all data:
- **Chat threads** - All conversations with complete history
- **Messages** - Every message from users and agents
- **Agent activities** - What each agent did in each thread (for debugging/tracing)
- **Eval suites** - Generated test cases
- **Test results** - Detailed test execution results

---

## ğŸ“Š Monitoring & Debugging

### In Streamlit UI (Recommended)

Open http://localhost:8501 and use the tabs:

1. **ğŸ’¬ Chat** - Interact with Seer
2. **ğŸ¤– Agent Threads** - Debug individual agent conversations:
   - View what each agent receives and sends
   - See tool calls and responses
   - Side-by-side comparison of CS and Eval agents
   - Filter by thread ID
3. **ğŸ›ï¸ Orchestrator Monitor** - Real-time message flow monitoring:
   - See all messages between agents
   - Track message broadcasting
   - View agent registration status
   - Monitor conversation threads

### LangGraph Studio (Browser-based)

Access LangGraph Studio for advanced debugging:
- **Customer Success Agent**: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:8001
- **Eval Agent**: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:8002

Features:
- Visual graph execution
- Real-time conversation monitoring
- Tool call inspection
- State management debugging
- A2A communication traces

### Via Log Files

```bash
# Orchestrator agent (LangGraph)
tail -f logs/orchestrator_langgraph.log

# Customer Success agent (LangGraph)
tail -f logs/customer_success_langgraph.log

# Eval agent (LangGraph)
tail -f logs/eval_agent_langgraph.log
```

**What you'll see in logs:**
- ğŸ›ï¸ Orchestrator activity and message broadcasting
- ğŸ¤– A2A communication traces between all agents
- ğŸ“¨ Agent-specific activity with `[ORCHESTRATOR]`, `[CS]`, or `[EVAL]` prefixes
- ğŸ”„ Agent registration and status updates

---

## ğŸ› ï¸ Development

**Add a new agent:**
1. Copy `agents/eval_agent/` as template
2. Define state, tools, and workflow
3. Register with Orchestrator agent (it will broadcast messages to you)
4. Update `run.py` to launch it

**Add new data types:**
1. Add schemas in `shared/schemas.py`
2. Update Orchestrator agent to handle the new data types
3. Update other agents to use Orchestrator for storage/retrieval

---

## ğŸ™ Built With

- **LangGraph** - All agents (Orchestrator, Customer Success, Eval)
- **LangChain** - LLM orchestration
- **Streamlit** - UI
- **OpenAI** - LLM models
- **SQLite** - Data persistence

---

**Questions? Check [README_FULL.md](README_FULL.md)** ğŸ”®

